{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leanear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests we will discuss about linear regression using gradient descent. The aim of this discussion is to learn the basics of both linear regression and gradient descent and math used behind it thatswhy no machine learning library is used in the code. So lets get started with the Linear regression,\n",
    "Linear regression is a basic and commonly used algorithm to estimate the relationship between dependent variable and one or more independent variables, those relations are linear (thats why the name linear regression) which is represented by the line. To represent the line we generally use the slope-intercept form whose equation is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = mx + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where m is the slope of the line and b is the y-intercept of the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/slope-intercept-form.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here x is independent variable and y is the dependet variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take an example of the gold prices which have been fairly linear in the year from 1970 to 2000 as seen in below graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/only_gold.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above datapoints, If we have to predict the gold value in 2005 then how would you predict it ? we can intuitively see that the relation is linear so we have to draw a line, so if roughly I imagine then I would draw a line something like,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/Final_output.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does our intuition worked here ? well, I would say \"I have drawn a line which is closest to all the points on the graph.\" Ok, using our intuition helped us here, but how to findout that nearly perfect line with using mathematics ? for that we have to know 2 things first is Error and the second is how to minimize the error. So lets talk about the first thing,\n",
    "Error - \n",
    "Is the difference between our predicted output and expected output, sometimes it is referred as the loss and for calculating loss different methods can be used. We have used the sum of squared error whose equation is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/SumOfSquare.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Yi means actual value\n",
    "\tm is the slope of the line\n",
    "\tb is the y intersept\n",
    "\tN is the number of the datapoints\n",
    "    So If we plot the graph of slope, y-intersept with the error then the graph will look something like,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/gradient_descent_error_surface.png\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have to findout the value of the m and b where error is minimum. (thats what we did intuitively) \n",
    "But while solving the problem we can not calculate the value of the error for every m and b as it will take exponential time so for finding those optimum values we have used an algorithm named gradient descent. \n",
    "(For understanding this algorithm you need to understand the calculus so I suggest you watch the Essense of calculus series by 3blue1brown on youtube) \n",
    "This algorithm is based on the concept that if we take the derivative of a curve at a certain point then the value we get is the slope of the line which is tangent to that curve. The slope value is known as gradient. Then with the gradient we decide whether to increase or decrease the input value. but if you have more than 1 variable in your loss function then to calculate the gradient with respect to each variable use partial derivatives.\n",
    "Here we have two variables in out equation one is m and second is b. So we need to calculate the gradient with respect to each of them with partial derivative which looks like,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/sum_of_square_gradient.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the gradients, update the values of the m and b. While updating if we subtract the values directly then our optimal values may miss so we need a multiplier, that multiplier is known as learning rate. we can define the learning rate as how much and how fast we are updating the values based on the gradient calculated. If the learning rate is too large then we may miss the optimal values of the m and b and if that is too small it will take much time to findout optimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the essential part is done, lets move towards the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b =  4.883462819066873  m =  146.503884572006\n",
      " predicted price is  5132.5194228392775\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VOXZ//HPFZZA2HfZQkD2RRQioFZFURRRsa1WNCouFfvU/mpbnz5qCVAB17ZutdWiqGCj1GqFiCgiiktVFKhINkiAbOwQCEvIfv/+mEONmJAEJpmZzPf9euU1M/ecM7nuJHO+OefMXGPOOUREJPxEBLoAEREJDAWAiEiYUgCIiIQpBYCISJhSAIiIhCkFgIhImFIAiIiEKQWAiEiYUgCIiISpxoEu4Hg6duzoYmJiAl2GiEhIWbNmzR7nXKfqlgvqAIiJiWH16tWBLkNEJKSYWVZNltMhIBGRMKUAEBEJUwoAEZEwpQAQEQlTCgARkTClABCR0JeQADExEBHhu0xICHRFISGoXwYqIlKthASYOhUKCny3s7J8twHi4gJXVwjQHoCIhLZp077d+B9VUOAbl+NSAIhIaMvOrt24/JcCQERCW3R07cblvxQAIhLaHngAoqK+OxYV5RuX41IAiEhoi4uDuXOhVy8w813OnasTwDWgVwGJSOiLi9MG/wRoD0BEJEwpAEREwpQCQEQkTCkARETClAJARCSY1GNfI70KSEQkWNRzXyPtAYiIBItp0zhU6phzwW0sOGOib6wO+xopAEREgoBzjreb92TcT59l3pmTyGnb5ds766ivkQ4BiYgEWOaew8xITObjSfcyeOcmnln0ICO2bfh2gTrqa6QAEBEJkMKSMp5ZuYlnPtpE00YRzOxawI1PT6Px4UPfLlSHfY0UACIiAbBywy5mJiaTtbeAK4Z3Y/rEQXRu3Qw6FvuO+Wdn+/7zf+CBOmtzoQAQEalH2/OPMHtJCkvX76BPxxb8/bbR/KBfx28XqMe+RgoAEZF6UFJWzvzPMnl8+UZKyx3/O74/t5/Xh8jGjQJWkwJARKSOrc7MI35REmk7DnLBgE7cf+VQojtEVb9iHVMAiIjUkbzDxTz8Tiqvrc6lW5tm/O3GkYwf3AUzC3RpgAJARMTvyssd/1idwyPvpnGosJQ7zu/DXeP6EdU0uDa5wVWNiEiIS96WT/yiJP6TvZ9Rvdsz56qh9O/SKtBlVUoBICLiBwcLS3hs+Ubmf5ZJu6im/Oma4fxoRPegOdxTGQWAiMhJcM6x5JvtzF6Swu5DRcSNjua34wfSJqpJoEurlgJAROQEbd59iBmLk/k0Yw/DurfhuZtiGd6zbaDLqrEaB4CZNQJWA1udc5ebWW9gIdAeWAvc6JwrNrNIYAEwEtgLXOucy/Qe4z7gNqAM+KVzbpk/JyMiUh8KS8r464cZPPvRZiIbRzBr0hDiRveiUUTwHu6pTG32AO4CUoHW3u1HgMedcwvN7Fl8G/ZnvMt9zrm+ZjbZW+5aMxsMTAaGAN2A982sv3OuzE9zERGpcx+m7WJGYhI5eUe46vRu/G7iIDq3ahbosk5IjdpBm1kPYCLwvHfbgAuB171F5gNXedcnebfx7h/nLT8JWOicK3LObQEygFH+mISISF3btv8Id7y8mlte+oqmjSJ45fbRPDH5jJDd+EPN9wCeAP4POPpapg7AfudcqXc7F+juXe8O5AA450rNLN9bvjvwRYXHrLiOiEhQKikr54VPt/DkinTKneO3lwzg9nP70LRx6H+cSrUBYGaXA7ucc2vMbOzR4UoWddXcd7x1Kn6/qcBUgOg66oEtIlITX27JI37RejbuPMRFgzoz84oh9Gwf+BYO/lKTPYBzgCvN7DKgGb5zAE8Abc2ssbcX0APY5i2fC/QEcs2sMdAGyKswflTFdf7LOTcXmAsQGxv7vYAQEalrew4V8dDSNN5Ym0v3ts157qZYLh7cpfoVQ0y1+zDOufuccz2cczH4TuJ+4JyLAz4ErvYWmwIs9q4nerfx7v/AOee88clmFum9gqgf8KXfZiIicpLKyx0Jq7IY96ePSFy3lZ+PPZXlvzmvQW784eTeB3APsNDM5gD/AeZ54/OAl80sA99//pMBnHPJZvYakAKUAnfqFUAiEiyStuYzbVES63L2M6aPr4VD387B2cLBX8z3z3lwio2NdatXrw50GSLSgB0oLOGx9zay4PNM2rdoSvzEwUw6vVtQt3Cojpmtcc7FVrec3gksImHJOUfium3MXpLK3sNF3DimF3ePH0Cb5sHfwsFfFAAiEnYydh1ixuIkPtu0l9N6tOHFm89kWI82gS6r3ikARCRsHCku4+kP05n78WaaNWnE7KuGcv2o6JBr4eAvCgARCQsrUncyMzGZ3H1H+NGI7tw3YRCdWkUGuqyAUgCISIOWu6+A+99KYXnKTvp1bsnCqWMY06dDoMsKCgoAEWmQikvLmffpFp5akQ7AvRMGcus5vRtECwd/UQCISIPz+aa9TF+cRMauQ4wf3IUZVwymR7uG08LBXxQAItJg7D5YxINLU3nzP1vp0a4586bEMm5Qw3wXrz8oAEQk5JWVO15ZlcWjyzZQWFLGLy7oy50X9KV500aBLi2oKQBEJKR9k7uf+EVJfJObz9mndmD2VUM5tVPLQJcVEhQAIhKS8o+U8MdlG/j7qiw6tozkycmnc+Xw0G7hUN8UACISUpxzLPp6Kw+8nUre4WKmnBXDb8b3p3Wz8Gnh4C8KABEJGek7DxK/KIlVW/IY3rMtL90yiqHdw6+Fg78oAEQk6BUUl/LnDzJ47uPNtIhszIM/HMbkM3sSEaYtHPxFASAiQe295B3c/1YKW/cf4eqRPbh3wkA6tgzvFg7+ogAQkaCUk1fA/W8l837qLgZ0acVrd5zFqN7tA11Wg6IAEJGgUlxaznOfbObPH6QTYcbvLhvILef0pkkjtXDwNwWAiASNzzL2MH1xEpt2H+bSIacw44rBdGvbPNBlNVgKABEJuF0HC3ng7VQWf72N6PZRvHjzmVwwsHOgy2rwFAAiEjBl5Y6/f5HFH5dtoKi0nF+O68fPx55KsyZq4VAfFAAiEhBf5+wnftF6krYe4Ad9OzJr0hD6qIVDvVIAiEi9yi8o4dFlabzyZTadWkby9PVnMHFYV7VwCAAFgIjUC+ccb6zdykNLU9lXUMwtZ/fm1xf3o5VaOASMAkBE6tzGnQeJfzOJLzPzGBHdlgW3jWJIN7VwCDQFgIjUmcNFpTy1Ip15n26hZbPGPPyjYfwkVi0cgoUCQET8zjnHsuSdzHormW35hVwb25N7JgykfYumgS5NKtBb60TCVUICxMRARITvMiHBLw+bvbeAW1/6ip/9fQ2tmzfh9Z+dxSNXn6aNfxDSHoBIOEpIgKlToaDAdzsry3cbIC7uhB6yqLSMuR9t5ukPM2gcYcRPHMTNZ8fQWC0cgpY55wJdQ5ViY2Pd6tWrA12GSMMTE+Pb6B+rVy/IzKz1w32avocZi5PYvOcwE4d1Jf7yQXRtoxYOgWJma5xzsdUtpz0AkXCUnV278SrsPFDI7CUpLPlmOzEdoph/6yjO79/JDwVKfVAAiISj6OjK9wCio2u0emlZOQs+z+Kx5RspLivnVxf142fnq4VDqFEAiISjBx747jkAgKgo33g11mbvI/7NJFK2H+C8/p2YdeUQYjq2qMNipa4oAETC0dETvdOm+Q77REf7Nv7HOQG873Axjy5L49UvczildTP+GjeCCUNPUQuHEKYAEAlXcXE1esVPebnj9bW5PPxOGvlHSrj93N7cdVF/WkZq8xHqqv0Nmlkz4GMg0lv+defcTDPrDSwE2gNrgRudc8VmFgksAEYCe4FrnXOZ3mPdB9wGlAG/dM4t8/+URMRf0nYcIP7NJFZn7WNkr3bMuWoog7q2DnRZ4ic1ifAi4ELn3CEzawJ8ambvAL8BHnfOLTSzZ/Ft2J/xLvc55/qa2WTgEeBaMxsMTAaGAN2A982sv3OurA7mJSIn4VBRKU++v5EX/p1J62aNefTq07h6RA+1cGhgqg0A53ujwCHvZhPvywEXAtd74/OB3+MLgEnedYDXgafNd5BwErDQOVcEbDGzDGAU8Lk/JiIiJ885xztJO5j1Vgo7DhRy3aie/N8lA2mnd/E2SDU6iGdmjYA1QF/gL8AmYL9zrtRbJBfo7l3vDuQAOOdKzSwf6OCNf1HhYSuuU/F7TQWmAkTX8CVpInLyMvccZmZiMh9t3M2grq356w0jGBHdLtBlSR2qUQB4h2lON7O2wJvAoMoW8y4r20d0xxk/9nvNBeaC753ANalPRE5cYUkZz360ib+u3ETTRhHMuHwwN53VSy0cwkCtTuM75/ab2UpgDNDWzBp7ewE9gG3eYrlATyDXzBoDbYC8CuNHVVxHRALg4427mbE4icy9BVx+WlemXz6YLq2bBbosqSfVRryZdfL+88fMmgMXAanAh8DV3mJTgMXe9UTvNt79H3jnERKByWYW6b2CqB/wpb8mIiI1tyO/kDsT1nLTC19iZrx82yievn6ENv5hpiZ7AF2B+d55gAjgNefcEjNLARaa2RzgP8A8b/l5wMveSd48fK/8wTmXbGavASlAKXCnXgEkUr9Ky8p56bNMHl++kdJyx90X92fq+X2IbKwWDuFI3UBFwsSarDymvZlE2o6DjB3QiVlXDiW6Q1Sgy5I6oG6gIgJA3uFiHnknjX+szqFrm2Y8e8NILhnSRS0cRAEg0lCVlzteW53Dw++mcaiwlDvO68Mvx/WjhVo4iEd/CSINUMq2A8QvWs/a7P2MimnP7KuGMuCUVoEuS4KMAkCkATlYWMLjy9OZ/3kmbZo34Y/XDOfHI7rrcI9USgEg0gA453h7/XZmL0lh18Eirh8VzW8vGUDbKLVwkKopAERC3JY9h5mxOIlP0vcwpFtrnr1hJGeohYPUgAJAJEQVlpTx15WbeHblJiIbR3D/lUO4YUwvGqljp9SQAkAkBH24YRe/T0wma28Bk07vxrTLBtFZ7+KVWlIAiISQbfuPMHtJCu8k7aBPpxa88tPRnN23Y6DLkhClABAJASVl5bz47y088X46ZeWO314ygJ+e21stHOSkKABEgtxXmXnEv5nEhp0HGTewM7+/cgg926uFg5w8BYBIkNp7qIiH30njn2ty6d62OXNvHMnFg9XCQfxHASASZMrLHQu/yuGRd9M4XFTK/4w9lf93YV+imurpKv6lvyiRIJK0NZ/4RUl8nbOf0b3bM+eqofTrohYOUjcUACJB4EBhCY+9t5EFn2fSvkVTHvvJcH54hlo4SN1SAIgEkHOOxHXbmPN2KnsOFXHD6F787/gBtIlqEujSJAwoAEQCZNPuQ8xYnMS/M/YyrHsb5k2J5bQebQNdloQRBYBIPSssKeMvH2bwt482E9kkgtmThnD9aLVwkPqnABCpRx+k7WRmYjI5eUf44Rnd+d1lg+jUKjLQZUmYUgCI1IOt+49wf2Iy76XspG/nlrx6+xjOOrVDoMuSMKcAEKlDJWXlzPt0C0++n47Dcc+lA7ntB71p2jgi0KWJKABE6sqqzXuJX5RE+q5DXDy4CzOvGEyPdmrhIMFDASDiZ3sOFfHg0lT+tXYrPdo15/mbYrlocJdAlyXyPQoAET8pK3e88mU2f3g3jSMlZdx5wan84oJ+NG+qjp0SnBQAIn6wPjef+EXrWZebz1l9OjD7qqH07dwy0GWJHJcCQOQk5B8p4bH3NvDyF1m0bxHJk5NP58rh3dTCQUKCAkDkBDjnWPy1r4VD3uEibjorhl9f3J82zdXCQUKHAkCkljJ2HWT6omQ+37yX4T3a8OLNZzKsR5tAlyVSawoAkRo6UlzGnz9I57mPMmhedIQ5K1/kun2pNGo3B+LiAl2eSK0pAERq4P0UXwuHrfuP8OPUj7jv/efoWJDvu3PqVN+lQkBCjAJA5Dhy8gq4/60U3k/dSf8uLfnHiscZvXrFdxcqKIBp0xQAEnIUACKVKC4t5/lPN/PUinQM474JA7n1B71pcvcHla+QnV2/BYr4gQJA5BifbdrD9EVJbNp9mEuGdGHGFUPo3ra5787oaMjK+v5K0dH1W6SIH1TbkcrMeprZh2aWambJZnaXN97ezJabWbp32c4bNzN7yswyzOwbMxtR4bGmeMunm9mUupuWSO3tOljIrxb+h+ufW0VxWTkv3BzL326M/XbjD/DAAxB1TD+fqCjfuEiIqckeQClwt3NurZm1AtaY2XLgZmCFc+5hM7sXuBe4B5gA9PO+RgPPAKPNrD0wE4gFnPc4ic65ff6elEhtlJU7ElZl8YdlGygqKeeXF/bl5xf0pVmTSlo4HD3OP22a77BPdLRv46/j/xKCqg0A59x2YLt3/aCZpQLdgUnAWG+x+cBKfAEwCVjgnHPAF2bW1sy6essud87lAXghcinwqh/nI1Ir63L2E78oifVb8zmnbwdmTRrKqZ2qaeEQF6cNvjQItToHYGYxwBnAKqCLFw4457abWWdvse5AToXVcr2xqsZF6l1+QQl/eC+NhFXZdGoZyZ+vO4PLT+uqFg4SVmocAGbWEngD+JVz7sBxniiV3eGOM37s95kKTAWI1ok18TPnHP9au5UHl6ayr6CYm8+O4TcX96dVM7VwkPBTowAwsyb4Nv4Jzrl/ecM7zayr999/V2CXN54L9Kyweg9gmzc+9pjxlcd+L+fcXGAuQGxs7PcCQuREbdx5kPhFSXy5JY8zotuy4LZRDOmmFg4SvmryKiAD5gGpzrnHKtyVCBx9Jc8UYHGF8Zu8VwONAfK9Q0XLgPFm1s57xdB4b0ykThUUl/LwO2lc9uQnbNhxkId+NIw3fna2Nv4S9mqyB3AOcCOw3sy+9sZ+BzwMvGZmtwHZwDXefUuBy4AMoAC4BcA5l2dms4GvvOVmHT0hLFIXnHO8l7KT+xOT2ZZfyE9ie3DPpQPp0DIy0KWJBIWavAroUyo/fg8wrpLlHXBnFY/1AvBCbQoUORE5eQXMTEzmg7RdDOjSin9edwZnxrQPdFkiQUXvBJYGpai0jOc+3syfP8igUYQx7bJB3HxODE0aVXu0UyTsKACkwfh3xh6mL05i8+7DXDbsFKZfPpiubZpXv6JImFIASMjbdaCQOW+nkrhuG706RPHSLWcydkDn6lcUCXMKAAlZpWXl/P2LLP703kaKSsu5a1w//mfsqZW3cBCR71EASEj6T/Y+4hclkbztAOf268isSUPp3bFFoMsSCSkKAAkp+wuKeeTdDSz8KpvOrSL5y/UjuGzYKWrhIHICFAASEpxzvL4ml4feSSP/SAm3ndObX13cn5aR+hMWOVF69kjQ27DjIPGL1vNV5j5GRLdlzlXDGNytdaDLEgl5CgAJWoeLSnlyRTrzPt1C62aNefTHp3H1yB5EROhwj4g/KAAk6DjneDdpB7OWpLA9v5DJZ/bknksH0q5F00CXJtKgKAAkqGTtPczMxGRWbtjNoK6tefr6EYzs1S7QZYk0SAoACQqFJWXM/Xgzf/kwg8YRxvTLBzPlrF40VgsHkTqjAJCA+yR9NzMWJ7Nlz2EuP60r8RMHc0qbZoEuS6TBUwBIwOw8UMisJSm8/c12YjpEseDWUZzXv1OgyxIJGwoAqXelZeXM/zyLx5dvpLisnF9f1J87zu+jFg4i9UwBIPVqTZavhUPq9gOc378TsyYNoVcHtXAQCQQFgNSLfYeLeeTdNBZ+lUPXNs149oYRXDJELRxEAkkBIHWqvPxoC4dUDhSWcvu5vfnVRf1poRYOIgGnZ6HUmdTtB4hflMSarH2cGdOO2VcNZeApauEgEiwUAOJ3h4pKeXz5Rl76LJM2zZvwh6t9LRx0uEckuOhdNuI3zjne/mY74/60khf+vYVrz+zJB3efzzWxPbFXXoGYGIiI8F0mJAS6XJGwpz0A8Ystew4zY3ESn6TvYXDX1jxzw0hGRHstHBISYOpUKCjw3c7K8t0GiIsLTMEigjnnAl1DlWJjY93q1asDXYYcR2FJGc+s3MQzH20islEEd4/vzw1jjmnhEBPj2+gfq1cvyMysr1JFwoaZrXHOxVa3nPYA5ISt3LCLmYnJZO0t4Mrh3YifOIjOrStp4ZCdXfkDVDUuIvVCASC1tj3/CLOXpLB0/Q76dGxBwk9Hc07fjlWvEB1d+R5AdHTdFSki1VIASI2VlJUz/7NMHl++kdJyx/+O78/t5/UhsnE1LRweeOC75wAAoqJ84yISMAoAqZHVmXnEL0oibcdBLhzYmfuvHELP9lE1W/noid5p03yHfaKjfRt/nQAWCSgFgBxX3uFiHlqayj/X5NKtTTP+duNIxg/uUvvX9MfFaYMvEmQUAFKp8nLHP1bn8Mi7aRwqLOWO8/tw17h+RDXVn4xIQ6Fns3xP0tZ84hcl8XXOfkb1bs+cq4bSv0urQJclIn6mAJD/OlhYwmPLNzL/s0zaRTXlsZ8M54dndFcLB5EGSgEgOOd465vtzFmSwu5DRcSNjua34wfSJqpJoEsTkTqkAAhzm3cfYsbiZD7N2MOw7m147qZYhvdsG+iyRKQeVNsMzsxeMLNdZpZUYay9mS03s3Tvsp03bmb2lJllmNk3ZjaiwjpTvOXTzWxK3UxHaqqwpIw/vbeBS5/4hHW5+5k1aQiL7jxHG3+RMFKTbqAvAZceM3YvsMI51w9Y4d0GmAD0876mAs+ALzCAmcBoYBQw82hoSP37MG0XFz/+EX/+IIOJp3Vlxd3nc9NZMTSK0LF+kXBS7SEg59zHZhZzzPAkYKx3fT6wErjHG1/gfB3mvjCztmbW1Vt2uXMuD8DMluMLlVdPegZSY1v3H2HWW8ksS97JqZ1a8Mrtozn71OO0cBCRBu1EzwF0cc5tB3DObTezzt54dyCnwnK53lhV41IPSsrKeeHTLTzxfjoOx28vGcDt5/ahaWN9HIRIOPP3SeDKjiG444x//wHMpuI7fES0moWdtFWb9zJ9cRIbdx7iokFdmHnF4Jq3cBCRBu1EA2CnmXX1/vvvCuzyxnOBnhWW6wFs88bHHjO+srIHds7NBeaC7/MATrC+sLfnUBEPLU3jjbW5dG/bnOduiuXiwV0CXZaIBJETDYBEYArwsHe5uML4L8xsIb4TvvleSCwDHqxw4nc8cN+Jly1VKSt3vPplNo++m8aRkjJ+PvZUfnFhX7VwEJHvqXarYGav4vvvvaOZ5eJ7Nc/DwGtmdhuQDVzjLb4UuAzIAAqAWwCcc3lmNhv4yltu1tETwuI/SVvzmbYoiXU5+xnTx9fCoW9ntXAQkcrpIyEbgPwjJTz23gZe/iKL9i0iiZ84iEmnd1MLB5EwpY+EDAPOORZ/vY05b6ey93ARN47pxd3jB9CmuVo4iEj1FAAhKmPXIaYvSuLzzXs5rUcbXrz5TIb1aBPoskQkhCgAQsyR4jKe/jCduR9vpnmTRsy5aijXjYrWu3hFpNYUACHk/ZSdzExMZuv+I/xoRHfumzCITq0iA12WiIQoBUAIyN1XwO8TU3g/dSf9Ordk4dQxjOnTIdBliUiIUwAEseLScp7/dDNPrUjHMO6dMJBbz+mtFg4i4hcKgCD1+SZfC4eMXYe4eLCvhUOPdmrhICL+owAIMrsPFvHg0lTe/M9WerRrzrwpsYwbpBYOIuJ/CoAgUVbueGVVFo8u20BhSRm/uKAvd17Ql+ZNGwW6NBFpoBQAQWBdzn7iFyWxfms+5/TtwKxJQzm1U8tAlyUiDZwCIIDyC0r4w3tpJKzKpmPLSJ667gyuOK2rWjiISL3Qy0kCwDnHv9bmMu6xlbyyKpspZ8Ww4u7zuXL4Mf17EhIgJgYiInyXCQmBKllEGiDtAdSz9J0HiV+UxKoteQzv2ZaXbhnF0O6VtHBISICpU6GgwHc7K8t3GyAurv4KFpEGS91A60lBcSlPrcjg+U820yKyMfdcOpDJZ/YkoqoWDjExvo3+sXr1gszMuixVREKcuoEGCecc76XsZNZbKWzdf4SrR/bgvgkD6dCymhYO2dm1GxcRqSUFQB3KySvg94nJrEjbxYAurXjtjrMY1bt9zVaOjq58D0CfkywifqIAqANFpWU8/8kW/vxBOhFmTLtsEDefE0OTRrU45/7AA989BwAQFeUbFxHxAwWAn/07Yw/TFyexefdhJgw9hemXD6Zb2+a1f6CjJ3qnTfMd9omO9m38dQJYRPxEAeAnuw4UMuftVBLXbSO6fRQv3nImFwzofHIPGhenDb6I1BkFwEkqK3e8/Hkmf3pvI0Wl5fxyXD9+PvZUmjVRCwcRCW4KgJPwdc5+pr25nuRtBzi3X0dmTRpK744tAl2WiEiNKABOwP6CYh5dtoFXv8ymU8tInr7+DCYOUwsHEQktCoBacM7xxtqtPLQ0lX0Fxdxydm9+fXE/WjVrEujSRERqTQFQQxt2HGT6oiS+zMxjRHRbFtw2iiHdKmnhICISIhQA1ThcVMpTK9KZ9+kWWjZrzCM/HsY1I4/TwkFEJEQoAKrgnGNZ8g7ufyuF7fmFXBvbk3smDKR9i6aBLk1ExC8aZjvok2yjnL23gFtf+oqf/X0tbZo34Y3/OYtHrj5NG38RaVAa3h7ASbRRLiot428fbeYvH2bQOMKInziIm8+OoXFtWjiIiISIhtcO+gTbKH+SvpsZi5PZsucwE4d1ZfrlgzmlTbPafW8RkSAQvu2ga9lGeeeBQmYvSWHJN9uJ6RDF/FtHcX7/TnVYoIhIcGh4AVDDNsqlZeUs+DyLx5ZvpLisnF9f1J87zu+jFg4iEjYaXgDUoI3y2ux9xL+ZRMr2A5zXvxOzrhxCjFo4iEiYaXgBcJw2yvsOF/PosjRe/TKHU1o345m4EVw69BS1cBCRsFTvAWBmlwJPAo2A551zD/v9mxzTRrm83PH6Vzk89E4qBwpLuf3c3tx1UX9aRja8/BMRqal63QKaWSPgL8DFQC7wlZklOudS6up7pm4/wPRFSazO2kdsr3bM+eFQBp7Suq6+nYhIyKjvf4FHARnOuc0AZrYQmAT4PQAOFZXyxPKNvPhZJq2bNebRq0/j6hE91MJBRMRT3wHQHcipcDsXGO3vb/JN7n6mLlhM62QfAAAGMklEQVTDjgOFXDeqJ/93yUDa6V28IiLfUd8BUNm/3995J5qZTQWmAkQf89LNmopuH0W/Li356w0jGBHd7oQeQ0SkoavvAMgFela43QPYVnEB59xcYC743gl8It+kbVRTXr7N7zsWIiINSn03ufkK6Gdmvc2sKTAZSKznGkREhHreA3DOlZrZL4Bl+F4G+oJzLrk+axAREZ96fyG8c24psLS+v6+IiHyX+hyLiIQpBYCISJhSAIiIhCkFgIhImFIAiIiEqaD+SEgz2w1U8ukuNdYR2OOncgKpocwDNJdg1FDmAZrLUb2cc9V+tGFQB8DJMrPVNflczGDXUOYBmkswaijzAM2ltnQISEQkTCkARETCVEMPgLmBLsBPGso8QHMJRg1lHqC51EqDPgcgIiJVa+h7ACIiUoWQCgAze8HMdplZUoWx4Wb2uZmtN7O3zKy1Nx5nZl9X+Co3s9O9+0Z6y2eY2VNmVu+fE+nHuaw0sw0V7uscxPNoYmbzvfFUM7uvwjqXevPIMLN763MOdTCXTG/8azNbHQJzaWpmL3rj68xsbIV1Qu25cry5BPq50tPMPvT+XpLN7C5vvL2ZLTezdO+ynTdu3s88w8y+MbMRFR5rird8uplNOeGinHMh8wWcB4wAkiqMfQWc712/FZhdyXrDgM0Vbn8JnIXvE8reASaE8FxWArGh8DsBrgcWetejgEwgBl9r8E1AH6ApsA4YHIpz8W5nAh0D9Ts5gbncCbzoXe8MrAEivNsh9VypZi6Bfq50BUZ411sBG4HBwKPAvd74vcAj3vXLvJ+5AWOAVd54e2Czd9nOu97uRGoKqT0A59zHQN4xwwOAj73ry4EfV7LqdcCrAGbWFWjtnPvc+X6aC4Cr6qbiqvljLsGglvNwQAszaww0B4qBA8AoIMM5t9k5VwwsBCbVde3H8tNcgkIt5zIYWOGttwvYD8SG6HOl0rnUQ5nVcs5td86t9a4fBFLxfU76JGC+t9h8vv0ZTwIWOJ8vgLbe7+QSYLlzLs85tw/f/C89kZpCKgCqkARc6V2/hu9+5ORR1/LtRrM7vo+mPCrXGwsGtZ3LUS96u7TTA7GLXomq5vE6cBjYDmQDf3TO5eH7+edUWD8UfidVzQV84fCema0x32dcB4uq5rIOmGRmjc2sNzDSuy8UnytVzeWooHiumFkMcAawCujinNsOvpDAt+cCVT8v/PZ8aQgBcCtwp5mtwbdbVVzxTjMbDRQ4544eP6z2g+kDqLZzAYhzzg0DzvW+bqyvYo+jqnmMAsqAbkBv4G4z60No/k6qmgvAOc65EcAEb93z6rnmqlQ1lxfwbURWA08AnwGlhObvpaq5QJA8V8ysJfAG8Cvn3PH2Gqv6+fvt91Lvnwjmb865NGA8gJn1ByYes8hkvvsfcy6+D6M/6nsfTB8oJzAXnHNbvcuDZvYKvg3TgrqvtmrHmcf1wLvOuRJgl5n9G9/ueQ7f/S8tFH4nVc1ls3Num7fuLjN7E9/v5OPvPXg9q2ouzrlS4NdHlzOzz4B0YB8h9lw5zlyC4rliZk3wbfwTnHP/8oZ3mllX59x27xDPLm88l8qfF7nA2GPGV55IPSG/B3D0TL6ZRQDxwLMV7ovAt3u48OiYt4t10MzGeLuANwGL67XoKtR2Lt5ubkfvehPgcny7xgF1nHlkAxd6r25oge/EVhq+E3r9zKy3mTXFF3SJ9V/599V2LmbWwsxaeeu0wLeRCvjvBKqei5lFebViZhcDpc65lFB8rlQ1l2B4rng/w3lAqnPusQp3JQJHX8kzhW9/xonATd7f2Bgg3/udLAPGm1k77xVD472x2gvUGfETPIv+Kr5jriX4UvA24C58Z9M3Ag/jvbnNW34s8EUljxOL75e/CXi64jqhNBegBb5XOXwDJANPAo2CdR5AS+CfXq0pwG8rPM5l3vKbgGnB/vdV1VzwvZJpnfeVHCJziQE24Dsp+T6+TpIh+Vypai5B8lz5Ab5DNd8AX3tflwEd8J24Tvcu23vLG/AX72e/ngqvYMJ3CCzD+7rlRGvSO4FFRMJUyB8CEhGRE6MAEBEJUwoAEZEwpQAQEQlTCgARkTClABARCVMKABGRMKUAEBEJU/8f6UxgUIxZYFEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#For plotiing the line on the graph\n",
    "def plotLines(b,m,axs,xi):\n",
    "    y = (np.array(np.array(xi)*m) + b)\n",
    "    plt.plot(np.array(axs),y)\n",
    "#For prediction of next price\n",
    "def predict_next_price(b,m,x):\n",
    "    return (x*m + b)\n",
    "#Minimizing the loss using gradienct descent algorithm\n",
    "def calculate_gradient_descent(x,y,learning_rate,iterations):\n",
    "    b = 0 #initial value of Y intercept\n",
    "    m = 0 #initial valur of slope of line\n",
    "    for iter_no in range(iterations): #going through all the data points multiple times\n",
    "        for i in range(len(x)):\n",
    "            b_gradient = 0\n",
    "            m_gradient = 0\n",
    "            x_current = x[i]\n",
    "            y_current = y[i]\n",
    "            N = len(x)\n",
    "            #formula for the partial derivative of error function with respect to b\n",
    "            b_gradient += -(2/N) * (y_current - ((m * x_current)+b)) \n",
    "            #formula for the partial derivative of error function with respect to m\n",
    "            m_gradient += -(2/N) * x_current * (y_current - ((m * x_current)+b))\n",
    "        #updating the values\n",
    "        b -= (learning_rate * b_gradient) \n",
    "        m -= (learning_rate * m_gradient)\n",
    "    return [b,m]\n",
    "\n",
    "def run():\n",
    "    #reading from csv file\n",
    "    year_price = pd.read_csv('GoldPricesIndia.csv')\n",
    "\n",
    "    axs = np.array(year_price[\"year\"])\n",
    "    #plotting the datapoints\n",
    "    plt.plot(axs,\"price\",\"ro\",data=year_price)\n",
    "    axs = np.array(axs) - 1970\n",
    "    #no. of times we have to go through data points\n",
    "    epochs = 1000\n",
    "\n",
    "    #here x_point,y_point - co-ordinates of the points from the dataset\n",
    "    #How fast the values should be updated\n",
    "    learning_rate = 0.0001\n",
    "    [b,m] = calculate_gradient_descent(np.array(axs), year_price[\"price\"] ,learning_rate, epochs)\n",
    "    print(\" b = \",b,\" m = \",m)\n",
    "    plotLines(b,m,np.array(axs)+1970,axs)\n",
    "    #For prediction\n",
    "    pred_year = 2005\n",
    "    pred_price = predict_next_price(b,m,pred_year-1970)\n",
    "    print(\" predicted price is \",pred_price)\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially when we have given the value to m and b as zero then graph looks like this,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/InitialStage.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orange line represents the line with 0 slope and 0 y-intercept. After which we have to update those values by calculating the gradients. So after the 1st iterationg over data points the gradients are,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b gradient = -1257.142857142857$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$m gradient = -37714.28571428571$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed we have to subtract the gradient values from the original values. you can understand the importance of the learning rate here if we directly subtract the gradients then our updated values will be far more than values which we are trying to optimize, and we will miss the optimal values by far margin. So while updating those values we will multiply the gradients with learning rate and then subtract from original values. So after 1st iteration our new values of b and m are,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b = 0.12571428571428572$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$m = 3.7714285714285714$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and our line looks like,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/after1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the line you can say that we have taken a small step in right direction, so after 50iterations our graph is,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/after50.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During those steps gradients will gradually decrese and eventually reaches close to zero, So we will stop when gradient approaches to zero or no. of epochs have reached."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "And finally after all the epochs we will get the line where error is minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/afterAllIteration.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the values now, we will use the equation y = mx + b where b and m will be final value given by gradient descent function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum it all,\n",
    "Linear regression is used to estimate the relation between a dependent variable with one or more independent variables.\n",
    "Error function gives us the difference between actual values and values predicted by our model\n",
    "To minimize the error we have used gradient descent algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
